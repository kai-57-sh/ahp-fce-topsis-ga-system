"""
Performance Benchmarking with Mathematical Validation

Research-grade performance testing with mathematical validation to ensure algorithms
maintain correctness while meeting performance requirements and complexity bounds.

Key Properties Validated:
- Time complexity verification through empirical testing
- Memory usage mathematical modeling
- Scalability limits mathematical determination
- Convergence rate analysis for optimization algorithms
- Performance regression prevention with mathematical bounds
"""

import pytest
import numpy as np
import time
import psutil
import os
import gc
from typing import Dict, List, Tuple, Any, Callable
from functools import wraps
import matplotlib.pyplot as plt
import json
from dataclasses import dataclass

# Add parent directory to path for imports
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from modules.topsis_module import topsis_rank
from modules.ahp_module import calculate_weights
from modules.evaluator import evaluate_batch
# from utils.performance_profiler import profile_function  # Commented out as module not available


@dataclass
class PerformanceMetrics:
    """Data class for performance measurement results."""
    execution_time: float
    memory_usage_mb: float
    input_size: int
    algorithm_name: str
    complexity_class: str
    mathematical_correctness: bool
    precision_error: float


class PerformanceBenchmarkSuite:
    """Comprehensive performance benchmarking suite with mathematical validation."""

    def __init__(self):
        self.results = []
        self.process = psutil.Process(os.getpid())

    def measure_performance(self, func: Callable, *args, **kwargs) -> PerformanceMetrics:
        """Measure execution time and memory usage of a function."""
        # Clear memory before measurement
        gc.collect()

        # Measure initial memory
        initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        # Measure execution time
        start_time = time.perf_counter()

        try:
            result = func(*args, **kwargs)
            execution_time = time.perf_counter() - start_time
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            result = None

        # Measure final memory
        final_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        memory_usage = final_memory - initial_memory

        # Determine input size (heuristic based on function)
        input_size = self._estimate_input_size(*args, **kwargs)

        # Mathematical correctness validation
        correctness, precision_error = self._validate_mathematical_correctness(
            func, result, *args, **kwargs
        )

        return PerformanceMetrics(
            execution_time=execution_time,
            memory_usage_mb=memory_usage,
            input_size=input_size,
            algorithm_name=func.__name__,
            complexity_class=self._determine_complexity_class(func),
            mathematical_correctness=correctness,
            precision_error=precision_error
        )

    def _estimate_input_size(self, *args, **kwargs) -> int:
        """Estimate the computational input size for complexity analysis."""
        size = 0
        for arg in args:
            if isinstance(arg, np.ndarray):
                size += arg.size
            elif isinstance(arg, (list, tuple)):
                size += len(arg)
            elif isinstance(arg, dict):
                size += len(arg)
            else:
                size += 1
        for value in kwargs.values():
            if isinstance(value, np.ndarray):
                size += value.size
            elif isinstance(value, (list, tuple)):
                size += len(value)
            elif isinstance(value, dict):
                size += len(value)
            else:
                size += 1
        return size

    def _validate_mathematical_correctness(self, func: Callable, result: Any, *args, **kwargs) -> Tuple[bool, float]:
        """Validate mathematical correctness of function results."""
        if result is None:
            return False, float('inf')

        try:
            if func.__name__ == 'topsis_rank':
                return self._validate_topsis_result(result, *args, **kwargs)
            elif func.__name__ == 'calculate_weights':
                return self._validate_ahp_result(result, *args, **kwargs)
            elif func.__name__ == 'evaluate_batch':
                return self._validate_evaluator_result(result, *args, **kwargs)
            else:
                # Generic validation
                return True, 0.0
        except Exception:
            return False, float('inf')

    def _validate_topsis_result(self, result: Dict, matrix: np.ndarray, weights: np.ndarray,
                               types: List[str], **kwargs) -> Tuple[bool, float]:
        """Validate TOPSIS mathematical correctness."""
        if not isinstance(result, dict) or 'Ci' not in result or 'rankings' not in result:
            return False, float('inf')

        ci_scores = result['Ci']
        rankings = result['rankings']

        # Mathematical validation
        try:
            # Property 1: Ci scores should be in [0, 1]
            ci_valid = all(0.0 <= ci <= 1.0 for ci in ci_scores)

            # Property 2: Rankings should be a permutation of 1..n
            n = len(ci_scores)
            expected_rankings = set(range(1, n + 1))
            rankings_valid = set(rankings) == expected_rankings

            # Property 3: Higher Ci should have better rank
            ranking_consistent = True
            for i in range(n):
                for j in range(n):
                    if ci_scores[i] > ci_scores[j] + 1e-10:
                        if rankings[i] >= rankings[j]:
                            ranking_consistent = False
                            break

            # Precision error estimation
            precision_error = max(
                max(abs(ci - 0.0) for ci in ci_scores if ci < 0.0),
                max(abs(ci - 1.0) for ci in ci_scores if ci > 1.0),
                0.0
            )

            return ci_valid and rankings_valid and ranking_consistent, precision_error

        except Exception:
            return False, float('inf')

    def _validate_ahp_result(self, result: Dict, matrix: np.ndarray, **kwargs) -> Tuple[bool, float]:
        """Validate AHP mathematical correctness."""
        if not isinstance(result, dict) or 'weights' not in result or 'CR' not in result:
            return False, float('inf')

        weights = result['weights']
        cr = result['CR']

        # Mathematical validation
        try:
            # Property 1: Weights should sum to 1.0
            weight_sum = sum(weights)
            sum_valid = abs(weight_sum - 1.0) < 1e-10

            # Property 2: All weights should be positive
            positivity_valid = all(w > 0 for w in weights)

            # Property 3: Consistency ratio should be reasonable
            cr_valid = 0.0 <= cr < 10.0  # Allow some flexibility

            # Precision error estimation
            precision_error = abs(weight_sum - 1.0) + max(0.0, -min(weights))

            return sum_valid and positivity_valid and cr_valid, precision_error

        except Exception:
            return False, float('inf')

    def _validate_evaluator_result(self, result: Dict, schemes: List[Dict], **kwargs) -> Tuple[bool, float]:
        """Validate evaluator mathematical correctness."""
        if not isinstance(result, dict) or 'individual_results' not in result:
            return False, float('inf')

        individual_results = result['individual_results']

        try:
            # Property 1: Should have results for all schemes
            count_valid = len(individual_results) == len(schemes)

            # Property 2: Each result should have valid mathematical structure
            structure_valid = True
            for scheme_id, scheme_result in individual_results.items():
                if not isinstance(scheme_result, dict):
                    structure_valid = False
                    break
                # Check for expected mathematical outputs
                if 'Ci' not in scheme_result or 'rank' not in scheme_result:
                    structure_valid = False
                    break

            # Precision error estimation (simplified)
            precision_error = 0.0  # Would need more detailed validation

            return count_valid and structure_valid, precision_error

        except Exception:
            return False, float('inf')

    def _determine_complexity_class(self, func: Callable) -> str:
        """Determine theoretical complexity class of function."""
        if func.__name__ == 'topsis_rank':
            return 'O(m*n)'  # m alternatives, n criteria
        elif func.__name__ == 'calculate_weights':
            return 'O(n^3)'  # n x n matrix eigenvalue computation
        elif func.__name__ == 'evaluate_batch':
            return 'O(k*m*n)'  # k schemes, m alternatives, n criteria
        else:
            return 'O(1)'


class TestTOPSISPerformance:
    """Performance testing for TOPSIS algorithm with mathematical validation."""

    def test_topsis_time_complexity(self):
        """Test TOPSIS time complexity through empirical measurement."""
        benchmark = PerformanceBenchmarkSuite()
        results = []

        # Test with increasing problem sizes
        alternatives_range = [5, 10, 20, 50, 100]
        criteria_count = 5  # Fixed

        for n_alternatives in alternatives_range:
            # Generate test data
            matrix = np.random.uniform(0.0, 10.0, (n_alternatives, criteria_count))
            weights = np.random.uniform(0.1, 1.0, criteria_count)
            weights = weights / sum(weights)
            types = ['benefit'] * criteria_count

            # Measure performance
            metrics = benchmark.measure_performance(
                topsis_rank, matrix, weights, types
            )
            results.append(metrics)

            # Mathematical correctness validation
            assert metrics.mathematical_correctness, \
                f"TOPSIS failed mathematical validation for {n_alternatives} alternatives"

            # Precision requirements
            assert metrics.precision_error < 1e-6, \
                f"TOPSIS precision error {metrics.precision_error} exceeds tolerance"

        # Analyze complexity
        self._analyze_complexity(results, 'O(m*n)')

        # Performance should be reasonable
        for metrics in results:
            if metrics.input_size <= 1000:  # Moderate size
                assert metrics.execution_time < 1.0, \
                    f"TOPSIS execution time {metrics.execution_time}s should be < 1s for moderate inputs"

    def test_topsis_memory_efficiency(self):
        """Test TOPSIS memory usage efficiency."""
        benchmark = PerformanceBenchmarkSuite()
        results = []

        # Test memory efficiency with different input sizes
        test_sizes = [(10, 5), (50, 10), (100, 20), (200, 30)]

        for n_alternatives, n_criteria in test_sizes:
            matrix = np.random.uniform(0.0, 10.0, (n_alternatives, n_criteria))
            weights = np.random.uniform(0.1, 1.0, n_criteria)
            weights = weights / sum(weights)
            types = ['benefit'] * n_criteria

            metrics = benchmark.measure_performance(
                topsis_rank, matrix, weights, types
            )
            results.append(metrics)

            # Memory usage should be proportional to input size
            expected_memory_ratio = metrics.input_size * 8 / 1024 / 1024  # Rough estimate
            actual_ratio = metrics.memory_usage_mb

            # Allow reasonable overhead (10x expected)
            assert actual_ratio < expected_memory_ratio * 10, \
                f"Memory usage {actual_ratio:.2f}MB seems excessive for input size {metrics.input_size}"

    def test_topsis_performance_regression(self):
        """Test for performance regressions in TOPSIS implementation."""
        benchmark = PerformanceBenchmarkSuite()

        # Baseline performance measurement
        matrix = np.random.uniform(0.0, 10.0, (50, 10))
        weights = np.random.uniform(0.1, 1.0, 10)
        weights = weights / sum(weights)
        types = ['benefit'] * 10

        # Run multiple measurements
        measurements = []
        for _ in range(5):
            metrics = benchmark.measure_performance(
                topsis_rank, matrix, weights, types
            )
            measurements.append(metrics.execution_time)

        # Performance should be consistent
        mean_time = np.mean(measurements)
        std_time = np.std(measurements)

        # Coefficient of variation should be low (< 20%)
        cv = std_time / mean_time if mean_time > 0 else float('inf')
        assert cv < 0.2, \
            f"TOPSIS performance inconsistent: CV = {cv:.3f}"

        # Mean performance should be reasonable
        assert mean_time < 0.1, \
            f"TOPSIS mean execution time {mean_time:.4f}s should be < 0.1s for 50x10 input"


class TestAHPerformance:
    """Performance testing for AHP algorithm with mathematical validation."""

    def test_ahp_time_complexity(self):
        """Test AHP time complexity through empirical measurement."""
        benchmark = PerformanceBenchmarkSuite()
        results = []

        # Test with increasing matrix sizes
        matrix_sizes = [3, 4, 5, 7, 10, 15]

        for n in matrix_sizes:
            # Generate consistent comparison matrix
            matrix = np.eye(n)
            for i in range(n):
                for j in range(i+1, n):
                    value = np.random.uniform(0.5, 2.0)
                    matrix[i, j] = value
                    matrix[j, i] = 1.0 / value

            # Measure performance
            metrics = benchmark.measure_performance(calculate_weights, matrix)
            results.append(metrics)

            # Mathematical correctness validation
            assert metrics.mathematical_correctness, \
                f"AHP failed mathematical validation for {n}x{n} matrix"

            # Precision requirements
            assert metrics.precision_error < 1e-10, \
                f"AHP precision error {metrics.precision_error} exceeds tolerance"

        # Analyze complexity (should be roughly O(n^3))
        self._analyze_complexity(results, 'O(n^3)')

    def test_ahp_memory_efficiency(self):
        """Test AHP memory usage efficiency."""
        benchmark = PerformanceBenchmarkSuite()

        # Test with larger matrices
        matrix_sizes = [5, 10, 15, 20]

        for n in matrix_sizes:
            matrix = np.eye(n)
            for i in range(n):
                for j in range(i+1, n):
                    value = np.random.uniform(0.5, 2.0)
                    matrix[i, j] = value
                    matrix[j, i] = 1.0 / value

            metrics = benchmark.measure_performance(calculate_weights, matrix)

            # Memory usage should scale reasonably with matrix size
            expected_memory = n * n * 8 / 1024 / 1024  # Matrix memory in MB
            assert metrics.memory_usage_mb < expected_memory * 100, \
                f"AHP memory usage {metrics.memory_usage_mb:.2f}MB seems excessive for {n}x{n} matrix"

    def test_ahp_consistency_validation_performance(self):
        """Test AHP consistency validation performance impact."""
        benchmark = PerformanceBenchmarkSuite()

        # Test with and without consistency validation
        n = 8
        matrix = np.eye(n)
        for i in range(n):
            for j in range(i+1, n):
                value = np.random.uniform(0.5, 2.0)
                matrix[i, j] = value
                matrix[j, i] = 1.0 / value

        # Measure with validation
        metrics_with_validation = benchmark.measure_performance(
            calculate_weights, matrix, validate_consistency=True
        )

        # Measure without validation
        metrics_without_validation = benchmark.measure_performance(
            calculate_weights, matrix, validate_consistency=False
        )

        # Validation should not significantly impact performance
        performance_ratio = metrics_with_validation.execution_time / metrics_without_validation.execution_time
        assert performance_ratio < 2.0, \
            f"Consistency validation should not double execution time: ratio = {performance_ratio:.2f}"


class TestEvaluatorPerformance:
    """Performance testing for batch evaluation with mathematical validation."""

    def test_batch_evaluation_scalability(self):
        """Test batch evaluation performance scalability."""
        benchmark = PerformanceBenchmarkSuite()
        results = []

        # Test with increasing numbers of schemes
        scheme_counts = [2, 5, 10, 20]

        for n_schemes in scheme_counts:
            # Generate test schemes
            test_schemes = []
            for i in range(n_schemes):
                scheme = {
                    'scheme_id': f'test_scheme_{i}',
                    'scheme_name': f'Test Scheme {i}',
                    'platform_inventory': {'USV': {'count': i + 1}},
                    'deployment_plan': {'primary_sector': {'coordinates': [0, 0], 'radius_km': 50}},
                    'task_assignments': {'test_task': {'primary_assets': ['USV'], 'coverage_requirement': 0.5}}
                }
                test_schemes.append(scheme)

            # Mock configurations to avoid file dependencies
            indicator_config = {
                'primary_capabilities': {},
                'secondary_indicators': {
                    'C1_1': {'name': 'Test1', 'primary_capability': 'C1', 'type': 'benefit', 'fuzzy_flag': False},
                    'C1_2': {'name': 'Test2', 'primary_capability': 'C1', 'type': 'benefit', 'fuzzy_flag': False}
                }
            }
            fuzzy_config = {
                'fuzzy_scale': {'Â∑Æ': 0.25, '‰∏≠': 0.5, 'ËâØ': 0.75, '‰ºò': 1.0},
                'applicable_indicators': ['C1_1', 'C1_2']
            }
            expert_judgments = {
                'primary_capabilities_file': 'mock_primary.yaml',
                'secondary_indicators_dir': 'mock_secondary_dir'
            }

            try:
                metrics = benchmark.measure_performance(
                    evaluate_batch, test_schemes, indicator_config, fuzzy_config, expert_judgments
                )
                results.append(metrics)

                # Mathematical correctness validation
                assert metrics.mathematical_correctness, \
                    f"Batch evaluation failed mathematical validation for {n_schemes} schemes"

            except Exception as e:
                # Some configurations might fail - acceptable for performance testing
                print(f"Batch evaluation failed for {n_schemes} schemes: {e}")

        if results:
            # Analyze scalability
            self._analyze_complexity(results, 'O(k*m*n)')

    def test_memory_usage_under_load(self):
        """Test memory usage under sustained load."""
        benchmark = PerformanceBenchmarkSuite()

        # Simulate sustained evaluation load
        memory_measurements = []
        test_iterations = 10

        for iteration in range(test_iterations):
            matrix = np.random.uniform(0.0, 10.0, (20, 8))
            weights = np.random.uniform(0.1, 1.0, 8)
            weights = weights / sum(weights)
            types = ['benefit'] * 8

            metrics = benchmark.measure_performance(
                topsis_rank, matrix, weights, types
            )
            memory_measurements.append(metrics.memory_usage_mb)

        # Memory usage should not grow significantly over time
        initial_memory = memory_measurements[0]
        final_memory = memory_measurements[-1]
        memory_growth = final_memory - initial_memory

        # Allow reasonable memory growth (< 10MB)
        assert memory_growth < 10.0, \
            f"Memory usage grew by {memory_growth:.2f}MB, indicating potential memory leak"

        # Memory usage should be reasonable
        max_memory = max(memory_measurements)
        assert max_memory < 50.0, \
            f"Peak memory usage {max_memory:.2f}MB seems excessive"

    def _analyze_complexity(self, results: List[PerformanceMetrics], expected_complexity: str):
        """Analyze empirical complexity against theoretical expectations."""
        if len(results) < 3:
            return  # Need sufficient data points

        # Extract input sizes and execution times
        sizes = [r.input_size for r in results]
        times = [r.execution_time for r in results]

        # Filter out zero or negative values
        valid_data = [(s, t) for s, t in zip(sizes, times) if s > 0 and t > 0]
        if len(valid_data) < 3:
            return

        sizes, times = zip(*valid_data)

        # Log-transform for power law analysis
        log_sizes = np.log(sizes)
        log_times = np.log(times)

        # Linear regression to find exponent
        coeffs = np.polyfit(log_sizes, log_times, 1)
        empirical_exponent = coeffs[0]

        # Validate against expected complexity (allow reasonable tolerance)
        complexity_mapping = {
            'O(1)': 0.0,
            'O(log n)': 0.1,
            'O(n)': 1.0,
            'O(n log n)': 1.1,
            'O(n^2)': 2.0,
            'O(n^3)': 3.0,
            'O(m*n)': 1.5,  # Approximate
            'O(k*m*n)': 2.0,  # Approximate
        }

        expected_exponent = complexity_mapping.get(expected_complexity, 1.0)
        tolerance = 0.5  # Allow reasonable tolerance for empirical measurement

        assert abs(empirical_exponent - expected_exponent) <= tolerance, \
            f"Complexity mismatch: empirical = {empirical_exponent:.2f}, expected ‚âà {expected_exponent:.2f} ({expected_complexity})"


class TestPerformanceRegressionPrevention:
    """Test suite for preventing performance regressions."""

    def test_performance_baseline_enforcement(self):
        """Enforce performance baselines for critical algorithms."""
        benchmark = PerformanceBenchmarkSuite()
        baselines = self._load_performance_baselines()

        # Test TOPSIS baseline
        matrix = np.random.uniform(0.0, 10.0, (100, 10))
        weights = np.random.uniform(0.1, 1.0, 10)
        weights = weights / sum(weights)
        types = ['benefit'] * 10

        topsis_metrics = benchmark.measure_performance(
            topsis_rank, matrix, weights, types
        )

        if 'topsis' in baselines:
            baseline = baselines['topsis']
            # Allow 20% performance degradation
            allowed_time = baseline['execution_time'] * 1.2
            assert topsis_metrics.execution_time < allowed_time, \
                f"TOPSIS performance regression: {topsis_metrics.execution_time:.4f}s > {allowed_time:.4f}s"

        # Test AHP baseline
        ahp_matrix = np.eye(8)
        for i in range(8):
            for j in range(i+1, 8):
                value = np.random.uniform(0.5, 2.0)
                ahp_matrix[i, j] = value
                ahp_matrix[j, i] = 1.0 / value

        ahp_metrics = benchmark.measure_performance(calculate_weights, ahp_matrix)

        if 'ahp' in baselines:
            baseline = baselines['ahp']
            allowed_time = baseline['execution_time'] * 1.2
            assert ahp_metrics.execution_time < allowed_time, \
                f"AHP performance regression: {ahp_metrics.execution_time:.4f}s > {allowed_time:.4f}s"

        # Store new baselines if they don't exist
        if 'topsis' not in baselines or 'ahp' not in baselines:
            self._store_performance_baseline('topsis', topsis_metrics)
            self._store_performance_baseline('ahp', ahp_metrics)

    def _load_performance_baselines(self) -> Dict[str, Dict]:
        """Load performance baselines from file."""
        baseline_file = os.path.join(os.path.dirname(__file__), '..', 'data', 'performance_baselines.json')

        if os.path.exists(baseline_file):
            with open(baseline_file, 'r') as f:
                return json.load(f)
        return {}

    def _store_performance_baseline(self, algorithm: str, metrics: PerformanceMetrics):
        """Store performance baseline to file."""
        baseline_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
        os.makedirs(baseline_dir, exist_ok=True)

        baseline_file = os.path.join(baseline_dir, 'performance_baselines.json')

        baselines = self._load_performance_baselines()
        baselines[algorithm] = {
            'execution_time': metrics.execution_time,
            'memory_usage_mb': metrics.memory_usage_mb,
            'input_size': metrics.input_size,
            'timestamp': time.time()
        }

        with open(baseline_file, 'w') as f:
            json.dump(baselines, f, indent=2)


# Utility functions
def run_comprehensive_performance_suite():
    """Run the complete performance benchmarking suite."""
    suite = TestTOPSISPerformance()

    print("üöÄ Running Comprehensive Performance Benchmarking Suite...")
    print("=" * 60)

    # TOPSIS Performance Tests
    print("üìä TOPSIS Performance Tests")
    try:
        suite.test_topsis_time_complexity()
        print("‚úÖ TOPSIS time complexity validation passed")
    except Exception as e:
        print(f"‚ùå TOPSIS time complexity validation failed: {e}")

    try:
        suite.test_topsis_memory_efficiency()
        print("‚úÖ TOPSIS memory efficiency validation passed")
    except Exception as e:
        print(f"‚ùå TOPSIS memory efficiency validation failed: {e}")

    try:
        suite.test_topsis_performance_regression()
        print("‚úÖ TOPSIS performance regression test passed")
    except Exception as e:
        print(f"‚ùå TOPSIS performance regression test failed: {e}")

    # AHP Performance Tests
    print("\n‚öñÔ∏è  AHP Performance Tests")
    ahp_suite = TestAHPerformance()

    try:
        ahp_suite.test_ahp_time_complexity()
        print("‚úÖ AHP time complexity validation passed")
    except Exception as e:
        print(f"‚ùå AHP time complexity validation failed: {e}")

    try:
        ahp_suite.test_ahp_memory_efficiency()
        print("‚úÖ AHP memory efficiency validation passed")
    except Exception as e:
        print(f"‚ùå AHP memory efficiency validation failed: {e}")

    try:
        ahp_suite.test_ahp_consistency_validation_performance()
        print("‚úÖ AHP consistency validation performance test passed")
    except Exception as e:
        print(f"‚ùå AHP consistency validation performance test failed: {e}")

    # Evaluator Performance Tests
    print("\nüîç Evaluator Performance Tests")
    evaluator_suite = TestEvaluatorPerformance()

    try:
        evaluator_suite.test_batch_evaluation_scalability()
        print("‚úÖ Batch evaluation scalability test passed")
    except Exception as e:
        print(f"‚ùå Batch evaluation scalability test failed: {e}")

    try:
        evaluator_suite.test_memory_usage_under_load()
        print("‚úÖ Memory usage under load test passed")
    except Exception as e:
        print(f"‚ùå Memory usage under load test failed: {e}")

    # Performance Regression Tests
    print("\nüìà Performance Regression Prevention")
    regression_suite = TestPerformanceRegressionPrevention()

    try:
        regression_suite.test_performance_baseline_enforcement()
        print("‚úÖ Performance baseline enforcement test passed")
    except Exception as e:
        print(f"‚ùå Performance baseline enforcement test failed: {e}")

    print("\n" + "=" * 60)
    print("üéØ Performance benchmarking suite completed!")
    print("Results stored in performance baselines for future regression detection.")


if __name__ == "__main__":
    run_comprehensive_performance_suite()